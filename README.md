
# Terminal LLM Chat (OpenAI & Together)
AS:This readme file is authomaticaly generated by ChatGPT5
AS:There are a lot of "useful flags" but I prefer to change models and their setting inside the code

A simple, dependency-light **terminal chat** script for talking to LLMs via the **OpenAI API** or **Together’s OpenAI-compatible API**.

- Read the **system prompt** and an optional **starter user prompt** from a **file** or **CLI flags**.
- The script **prints both prompts** and asks you to **continue the starter prompt** before the first request.
- Supports **streaming**, **history**, **logging**, and optional stripping of `<think>...</think>` blocks from terminal output (useful for models like DeepSeek R1).

> File → `term_chat.py`  
> Python ≥ 3.9

---

## Features

- **Two prompt sources:** file (preferred) or `--system` / `--user` flags.
- **First-turn continuation:** the program shows your prompts and asks you to *continue* the starter user prompt.
- **OpenAI or Together:** choose provider with `--provider`; Together works via `base_url=https://api.together.xyz/v1`.
- **Streaming output** by default (disable with `--no-stream`).
- **Think-block filtering:** hide `<think>...</think>` in terminal via `--strip-think` (while **keeping them in logs**).
- **Logging:** markdown transcripts with system prompt, starter prompt (after continuation), and every turn (raw).

---

## Installation

```bash
# clone your repo
git clone https://github.com/YOURNAME/YOURREPO.git
cd YOURREPO

# (optional) create a venv
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

# install deps
pip install openai python-dotenv
````

> `python-dotenv` is optional but recommended; it lets the script read API keys from a `.env` file.

---

## Configuration

Create a `.env` file in the project root (or any parent directory):

```dotenv
OPENAI_API_KEY=sk-...
TOGETHER_API_KEY=xxxxxxxxxxxxxxxx
```

You can set one or both. The script will use the right one depending on `--provider`.

---

## Prompt File Format

Preferred file format uses two headers:

```
===SYSTEM===
<system prompt here>
===USER===
<starter user prompt here>
```

**Fallbacks supported:**

* A single line of `---` as a separator between system and user sections, **or**
* No headers at all: the whole file becomes the starter user prompt, and the system prompt is empty.

### Example: `myprompts.txt`

```
===SYSTEM===
You are a precise but friendly research assistant.
Always ask clarifying questions if the user’s goal is ambiguous.
Prefer concise bullet points over long paragraphs unless asked.

===USER===
Let’s outline a 2-week study plan for the LASR skills assessment.
Focus on hands-on tasks (daily), short theory refreshers, and checkpoints.
```

---

## Usage

Basic help:

```bash
python term_chat.py -h
```

### With a prompt file (recommended)

```bash
python term_chat.py myprompts.txt --provider together --model meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
```

The program will:

1. Print the **system** and **starter user** prompts from the file,
2. Ask you to **continue** the starter user prompt (multi-line; finish with an empty line),
3. Send the first request and stream the reply,
4. Enter a chat loop (`/exit` to quit).

### Without a prompt file (CLI prompts)

```bash
python term_chat.py \
  --system "You are terse and helpful." \
  --user "Draft a 3-point plan to benchmark LLMs." \
  --provider openai \
  --model gpt-4o-mini
```

### Useful flags

* `--provider {openai,together}` — default: `openai`
* `--model MODEL_NAME` — default depends on provider (see “Defaults”)
* `--temperature FLOAT` — sampling temperature
* `--max-tokens INT` — per assistant reply (default: 1024)
* `--no-stream` — disable token streaming
* `--strip-think` — hide `<think>...</think>` in terminal (still stored in history & logs)
* `--system "..."` — system prompt override
* `--user "..."` — starter user prompt override

**Exit the chat loop:** type `/exit` or `/quit`.

---

## Logging

* If you launched with a file like **`myprompts.txt`**, logs go to **`myprompts_logs/`**.
* If you launched **without a file**, logs go to **`logs/`**.
* Each run creates a Markdown log file named:

```
YYYYmmdd_HHMMSS_provider_model.md
```

Each log contains:

* Run metadata (provider, model, temperature, streaming, etc.)
* **Raw prompt file content** (if a file was used)
* **System prompt**
* **Starter user prompt (after your continuation)**
* **Every user/assistant turn** as `### role` fenced blocks
  (assistant turns are stored **raw**, including `<think>` blocks if present)

> ⚠️ **Privacy note:** logs can contain sensitive inputs and full model outputs. Treat the `*_logs/` folders as private unless you scrub them.

---

## Defaults

At the top of `term_chat.py` you’ll find edit-friendly defaults:

```python
DEFAULT_PROVIDER = "openai"  # "openai" or "together"
DEFAULT_OPENAI_MODEL = "gpt-4o-mini"
DEFAULT_TOGETHER_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 1024
STREAM_BY_DEFAULT = True
```

Change them to your preferred setup, or override with CLI flags.

---

## Examples

OpenAI, streaming (default):

```bash
python term_chat.py myprompts.txt --provider openai --model gpt-4o-mini
```

Together, hide `<think>` in terminal:

```bash
python term_chat.py myprompts.txt \
  --provider together \
  --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B \
  --strip-think
```

No file; prompts via CLI:

```bash
python term_chat.py \
  --system "You are a code reviewer." \
  --user "Review this function for edge cases." \
  --provider openai --model gpt-4o-mini
```

---

## Troubleshooting

* **`Missing OPENAI_API_KEY` / `Missing TOGETHER_API_KEY`**
  Ensure `.env` exists and contains the right key for the chosen provider.

* **`ModuleNotFoundError: openai`**
  Run `pip install openai python-dotenv`.

* **401/403/404 errors with Together**
  Double-check your Together API key and model name. The script already sets
  `base_url="https://api.together.xyz/v1"` for compatibility.

* **Nothing appears during streaming**
  Some terminals buffer output differently; try without streaming (`--no-stream`) to confirm responses arrive.

---

## Security & Notes

* The script does **not** log API keys.
* Logs may include confidential inputs/outputs—handle accordingly.
* `<think>` blocks (if any) are **kept in logs** and history even when `--strip-think` is used.
